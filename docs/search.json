[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Python-R Dictionary for Data Science",
    "section": "",
    "text": "I personally consider R and Python to be two very powerful programming languages (and ecosystems) to make data science. Being mainly an R user for three years, I had to learn (quickly) Python and its modules for data science due to a change of career. Starting to learn a programming language from scratch and getting familiar with the APIs of lots of packages can be overwhelming. Hence, I started looking for similarities across the R and Python ecosystems and I’ve decided to create this book as a personal reference to be able to switch from one language to another seamlessly.\nThis book must not be intended as an in-depth guide into any of the concepts exposed. Instead, it should be considered as a dictionary aimed at translating many common data science problems from R to Python and viceversa. The current version of this dictionary covers the following macro-areas of data science:\n\nData Collection\nData Manipulation\nData Visualization\nMachine Learning\n\nObviously, data science is a broader topic and you can find links to external useful resources throughout the book. I chose to translate the most common and useful commands with code snippets which needed to be both concise and as accurate as possible.\nThis project was created with the language-agnostic publishing system Quarto, which can be considered as a younger (yet very ambitious) brother of rmarkdown.\nHopefully, this resource may help someone else in their journey through data science with R and Python. Enjoy."
  },
  {
    "objectID": "chapters/p00_intro.html",
    "href": "chapters/p00_intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Before taking your bilingual journey through data science, I want to highlight some useful resources which can improve your overall learning experience."
  },
  {
    "objectID": "chapters/p00_intro.html#choose-an-ide",
    "href": "chapters/p00_intro.html#choose-an-ide",
    "title": "1  Introduction",
    "section": "1.1 Choose an IDE",
    "text": "1.1 Choose an IDE\nMaking data science in the command line is ok, but you can save time and improve a lot your workflow by using an integrated development environment (IDE). You will have the ability to create scripts, running code interactively, making plots, debugging code and more within a single application.\nIn my opinion, the RStudio IDE is the best solution out there for R at present.\nFor Python, good choices are Spyder, PyCharm and Jupyter."
  },
  {
    "objectID": "chapters/p00_intro.html#learn-by-others-and-improve-yourself",
    "href": "chapters/p00_intro.html#learn-by-others-and-improve-yourself",
    "title": "1  Introduction",
    "section": "1.2 Learn by others and improve yourself",
    "text": "1.2 Learn by others and improve yourself\n\n1.2.1 Kaggle\nKaggle is an online platform hosting datasets and competitions aimed at solving real-life problems. This is one of the best places to getting your hands dirty in data science!\nThroughout the book, we will use many (?) Kaggle datasets. You can install the CLI of Kaggle API and run the following command on bash to download a dataset:\n\nkaggle datasets download kamilpytlak/personal-key-indicators-of-heart-disease ./data --unzip\n\n\n\n1.2.2 TidyTuesday\nAs described in the main page, TidyTuesday is a weekly social data project aimed at applying your R skills, getting feedback, exploring other’s work and connecting with the greater R community. Every week (yes, on Tuesday), a new dataset is posted on the GitHub page and people are encouraged to produce useful insights from it (usually) through figures. Once you’re done with your visualization, you can post it on Twitter by using the hashtags #TidyTuesday and #RStats. It is also recommended to share your code and adding alt text to your visualizations.\n\n\n1.2.3 Big Books\nThere will be lots of links to external resources throughout the book. Anyway, I suggest you to take a look at these two meta-books which can help you to point to the right direction in your learning path:\n\nthe Big Book of R;\nthe Big Book of Python."
  },
  {
    "objectID": "chapters/p00_intro.html#integrate-r-and-python",
    "href": "chapters/p00_intro.html#integrate-r-and-python",
    "title": "1  Introduction",
    "section": "1.3 Integrate R and Python",
    "text": "1.3 Integrate R and Python\nIf you want to integrate Python code in your R projects, I suggest you to take a look at the R package reticulate.\nFor example, it is possible to import Python libraries and use their functions to create R objects. In the following code chunk, you can see how we import Numpy and Pandas (using R syntax), create two Numpy arrays (R vectors) and use them to make a Pandas DataFrame (R data.frame):\n\nlibrary(reticulate)\n\nnp <- import(\"numpy\")\npd <- import(\"pandas\")\n\na1 <- np$array(c(1, 2, 3, 4))\na2 <- np$array(5:8)\npd$DataFrame(list(a1 = a1, a2 = a2), index = letters[1:4])\n\n  a1 a2\na  1  5\nb  2  6\nc  3  7\nd  4  8\n\n\nYou can also use Python interactively through the R console (reticulate::repl_python() function) or sourcing Python scripts (reticulate::source_python()) and more."
  },
  {
    "objectID": "chapters/p01_dc.html",
    "href": "chapters/p01_dc.html",
    "title": "Data Collection",
    "section": "",
    "text": "The following chapters will show you how to import data into R and Python from several sources using different packages:\n\nreadr and Pandas for the most common tabular data formats;\nreadxl and openpyxl for Excel files;\njsonlite and json for JSON files;\nxml2 and ??? for XML files;\nDBI and ??? for SQL databases;\nrvest and Beautiful Soup for web scraping;\nhttr2 and Requests to interrogate APIs."
  },
  {
    "objectID": "chapters/dc_structured.html",
    "href": "chapters/dc_structured.html",
    "title": "2  Structured Data",
    "section": "",
    "text": "The most common tabular (or structured) file formats one may encounter in a data science project are:\nWe will consider the Personal key indicators of heart disease from Kaggle."
  },
  {
    "objectID": "chapters/dc_structured.html#delimited-files",
    "href": "chapters/dc_structured.html#delimited-files",
    "title": "2  Structured Data",
    "section": "2.1 Delimited files",
    "text": "2.1 Delimited files\n\n2.1.1 TSV\nAs an example, let’s consider the Animal Crossing dataset from week 19 of the 2020 TidyTuesday challenge.\n\nRPython\n\n\n\nurl <- \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/critic.tsv\"\n\ndf <- read_tsv(url, show_col_types = FALSE)\ndf\n\n# A tibble: 107 × 4\n   grade publication      text                                        date      \n   <dbl> <chr>            <chr>                                       <date>    \n 1   100 Pocket Gamer UK  Animal Crossing; New Horizons, much like i… 2020-03-16\n 2   100 Forbes           Know that if you’re overwhelmed with the w… 2020-03-16\n 3   100 Telegraph        With a game this broad and lengthy, there’… 2020-03-16\n 4   100 VG247            Animal Crossing: New Horizons is everythin… 2020-03-16\n 5   100 Nintendo Insider Above all else, Animal Crossing: New Horiz… 2020-03-16\n 6   100 Trusted Reviews  Animal Crossing: New Horizons is the best … 2020-03-16\n 7   100 VGC              Nintendo's comforting life sim is a tranqu… 2020-03-16\n 8   100 God is a Geek    A beautiful, welcoming game that is everyt… 2020-03-16\n 9   100 Nintendo Life    Animal Crossing: New Horizons takes Animal… 2020-03-16\n10   100 Daily Star       Similar to how Breath of the Wild and Odys… 2020-03-16\n# … with 97 more rows\n\n\n\n\n\nurl = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/critic.tsv\"\n\ndf = pd.read_table(url)\ndf\n\n     grade  ...        date\n0      100  ...  2020-03-16\n1      100  ...  2020-03-16\n2      100  ...  2020-03-16\n3      100  ...  2020-03-16\n4      100  ...  2020-03-16\n..     ...  ...         ...\n102     90  ...  2020-04-16\n103     90  ...  2020-04-17\n104     95  ...  2020-04-22\n105     90  ...  2020-05-01\n106     80  ...  2020-05-01\n\n[107 rows x 4 columns]\n\n\n\n\n\n\n\n2.1.2 CSV\nAs an example, let’s consider the Indoor Pollution dataset from week 15 of the 2022 TidyTuesday challenge.\n\nRPython\n\n\n\nurl <- \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-04-12/indoor_pollution.csv\"\n\nread_csv(url, show_col_types = FALSE)\n\n# A tibble: 8,010 × 4\n   Entity      Code   Year `Deaths - Cause: All causes - Risk: Household air p…`\n   <chr>       <chr> <dbl>                                                 <dbl>\n 1 Afghanistan AFG    1990                                                  19.6\n 2 Afghanistan AFG    1991                                                  19.3\n 3 Afghanistan AFG    1992                                                  19.5\n 4 Afghanistan AFG    1993                                                  19.7\n 5 Afghanistan AFG    1994                                                  19.4\n 6 Afghanistan AFG    1995                                                  19.6\n 7 Afghanistan AFG    1996                                                  19.8\n 8 Afghanistan AFG    1997                                                  19.7\n 9 Afghanistan AFG    1998                                                  19.0\n10 Afghanistan AFG    1999                                                  19.9\n# … with 8,000 more rows\n\n\n\n\n\nurl = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-04-12/indoor_pollution.csv\"\n\npd.read_csv(url)\n\n           Entity  ... Deaths - Cause: All causes - Risk: Household air pollution from solid fuels - Sex: Both - Age: Age-standardized (Percent)\n0     Afghanistan  ...                                          19.623001                                                                       \n1     Afghanistan  ...                                          19.335193                                                                       \n2     Afghanistan  ...                                          19.508785                                                                       \n3     Afghanistan  ...                                          19.677607                                                                       \n4     Afghanistan  ...                                          19.432528                                                                       \n...           ...  ...                                                ...                                                                       \n8005     Zimbabwe  ...                                           8.603630                                                                       \n8006     Zimbabwe  ...                                           8.656817                                                                       \n8007     Zimbabwe  ...                                           8.690100                                                                       \n8008     Zimbabwe  ...                                           8.736245                                                                       \n8009     Zimbabwe  ...                                           8.658818                                                                       \n\n[8010 rows x 4 columns]\n\n\n\n\n\n\n\n2.1.3 General delimiter\nAgain, we can consider the Animal Crossing dataset from TidyTuesday and use the read_table function to import the data by specifying a separator/delimiter.\n\nRPython\n\n\n\nurl <- \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/critic.tsv\"\n\nread_delim(url, delim =\"\\t\", show_col_types = FALSE)\n\n# A tibble: 107 × 4\n   grade publication      text                                        date      \n   <dbl> <chr>            <chr>                                       <date>    \n 1   100 Pocket Gamer UK  Animal Crossing; New Horizons, much like i… 2020-03-16\n 2   100 Forbes           Know that if you’re overwhelmed with the w… 2020-03-16\n 3   100 Telegraph        With a game this broad and lengthy, there’… 2020-03-16\n 4   100 VG247            Animal Crossing: New Horizons is everythin… 2020-03-16\n 5   100 Nintendo Insider Above all else, Animal Crossing: New Horiz… 2020-03-16\n 6   100 Trusted Reviews  Animal Crossing: New Horizons is the best … 2020-03-16\n 7   100 VGC              Nintendo's comforting life sim is a tranqu… 2020-03-16\n 8   100 God is a Geek    A beautiful, welcoming game that is everyt… 2020-03-16\n 9   100 Nintendo Life    Animal Crossing: New Horizons takes Animal… 2020-03-16\n10   100 Daily Star       Similar to how Breath of the Wild and Odys… 2020-03-16\n# … with 97 more rows\n\n\n\n\n\nurl = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/critic.tsv\"\n\npd.read_table(url, sep=\"\\t\")\n\n     grade  ...        date\n0      100  ...  2020-03-16\n1      100  ...  2020-03-16\n2      100  ...  2020-03-16\n3      100  ...  2020-03-16\n4      100  ...  2020-03-16\n..     ...  ...         ...\n102     90  ...  2020-04-16\n103     90  ...  2020-04-17\n104     95  ...  2020-04-22\n105     90  ...  2020-05-01\n106     80  ...  2020-05-01\n\n[107 rows x 4 columns]"
  },
  {
    "objectID": "chapters/dc_structured.html#excel-files",
    "href": "chapters/dc_structured.html#excel-files",
    "title": "2  Structured Data",
    "section": "2.2 Excel files",
    "text": "2.2 Excel files\nAt the moment, readxl does not support reading files from URLs. Hence, we will consider an example dataset which comes into the readxl package.\n\nRPython\n\n\n\nlibrary(readxl)\n\npath_to_ds <- readxl_example(\"clippy.xlsx\")\nread_xlsx(path_to_ds)\n\n# A tibble: 4 × 2\n  name                 value    \n  <chr>                <chr>    \n1 Name                 Clippy   \n2 Species              paperclip\n3 Approx date of death 39083    \n4 Weight in grams      0.9      \n\n\n\n\n\npd.read_excel(r.path_to_ds)\n\n                   name                value\n0                  Name               Clippy\n1               Species            paperclip\n2  Approx date of death  2007-01-01 00:00:00\n3       Weight in grams                  0.9"
  },
  {
    "objectID": "chapters/dc_structured.html#summarize-your-data",
    "href": "chapters/dc_structured.html#summarize-your-data",
    "title": "2  Structured Data",
    "section": "2.3 Summarize your data",
    "text": "2.3 Summarize your data\n\n\n\n\n\n\nThe skimr package\n\n\n\nThe skimr package can be very useful to easily summarize your data!\n\n\n\nRPython\n\n\n\nskimr::skim(df)\n\n\nData summary\n\n\nName\ndf\n\n\nNumber of rows\n107\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nDate\n1\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\npublication\n0\n1\n2\n29\n0\n107\n0\n\n\ntext\n0\n1\n22\n833\n0\n107\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\ndate\n0\n1\n2020-03-16\n2020-05-01\n2020-03-23\n28\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ngrade\n0\n1\n90.64\n6.11\n70\n90\n90\n94\n100\n▁▂▁▇▃\n\n\n\n\nsummary(df)\n\n     grade        publication            text                date           \n Min.   : 70.00   Length:107         Length:107         Min.   :2020-03-16  \n 1st Qu.: 90.00   Class :character   Class :character   1st Qu.:2020-03-16  \n Median : 90.00   Mode  :character   Mode  :character   Median :2020-03-23  \n Mean   : 90.64                                         Mean   :2020-03-25  \n 3rd Qu.: 94.00                                         3rd Qu.:2020-04-02  \n Max.   :100.00                                         Max.   :2020-05-01  \n\n\n\n\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 107 entries, 0 to 106\nData columns (total 4 columns):\n #   Column       Non-Null Count  Dtype \n---  ------       --------------  ----- \n 0   grade        107 non-null    int64 \n 1   publication  107 non-null    object\n 2   text         107 non-null    object\n 3   date         107 non-null    object\ndtypes: int64(1), object(3)\nmemory usage: 3.5+ KB\n\ndf.describe()\n\n            grade\ncount  107.000000\nmean    90.635514\nstd      6.114308\nmin     70.000000\n25%     90.000000\n50%     90.000000\n75%     94.000000\nmax    100.000000"
  },
  {
    "objectID": "chapters/dc_semi-structured.html",
    "href": "chapters/dc_semi-structured.html",
    "title": "3  Semi-structured Data",
    "section": "",
    "text": "When we talk about semi-structured data we usually refer to data that is not represented as a table, but as a hierarchical nested structure of key-value pairs. We will see how to import two kinds of file:"
  },
  {
    "objectID": "chapters/dc_semi-structured.html#json",
    "href": "chapters/dc_semi-structured.html#json",
    "title": "3  Semi-structured Data",
    "section": "3.1 JSON",
    "text": "3.1 JSON\nWe will use the Open Library RESTful API (see chapter XX to know more about APIs) in order to retrieve information about books written by one of my favorite authors, Anna Maria Ortese.\n\nRPython\n\n\n\njson_data <- read_json(\"http://openlibrary.org/search.json?author=anna+maria+ortese\",\n                       simplifyVector = TRUE)\n\n\n\n\njson_data = pd.read_json(\"http://openlibrary.org/search.json?author=anna+maria+ortese\")\n\n\n\n\nBy default, jsonlite::read_json() will not simplify the JSON structure, whereas pd.read_json() will try to convert the JSON file into a table.\n\nlapply(json_data, length) # in R\n\n$numFound\n[1] 1\n\n$start\n[1] 1\n\n$numFoundExact\n[1] 1\n\n$docs\n[1] 56\n\n$num_found\n[1] 1\n\n$q\n[1] 1\n\n$offset\n[1] 0\n\n\nWe are interested in retrieving information from four sub-fields coming from the docs field:\n\nRPython\n\n\n\njson_data$docs |> \n    dplyr::select(author_name, title, publish_year, number_of_pages_median) |> \n    head(5)\n\n        author_name                    title                       publish_year\n1 Anna Maria Ortese Il mare non bagna Napoli 1955, 1979, 1953, 1994, 1975, 1967\n2 Anna Maria Ortese                L' iguana                         1978, 1965\n3 Anna Maria Ortese                  Romanzi                               2002\n4 Anna Maria Ortese        Poveri e semplici                         1974, 1967\n5 Anna Maria Ortese            Short Stories                               1994\n  number_of_pages_median\n1                    160\n2                    204\n3                     NA\n4                    163\n5                    191\n\n\n\n\n\n(\n    pd.json_normalize(json_data[\"docs\"])\n    .loc[:,[\"author_name\", \"title\", \"publish_year\", \"number_of_pages_median\"]]\n    .head(5)\n)\n\n           author_name  ... number_of_pages_median\n0  [Anna Maria Ortese]  ...                  160.0\n1  [Anna Maria Ortese]  ...                  204.0\n2  [Anna Maria Ortese]  ...                    NaN\n3  [Anna Maria Ortese]  ...                  163.0\n4  [Anna Maria Ortese]  ...                  191.0\n\n[5 rows x 4 columns]\n\n\n\n\n\nRead more about how to subset data frames in Chapter 5"
  },
  {
    "objectID": "chapters/dc_semi-structured.html#xml",
    "href": "chapters/dc_semi-structured.html#xml",
    "title": "3  Semi-structured Data",
    "section": "3.2 XML",
    "text": "3.2 XML\n\nRPython\n\n\n\nxml_data <- read_xml(\"https://www.w3schools.com/xml/simple.xml\")\n\n\n\n\nxml_data = pd.read_xml(\"https://www.w3schools.com/xml/simple.xml\")\n\n\n\n\n\nRPython\n\n\n\nxml_data |> \n    xml_children() |> \n    as_list() |> \n    dplyr::bind_rows() |> \n    tidyr::unnest(cols = c(name, price, description, calories))\n\n# A tibble: 5 × 4\n  name                        price description                         calories\n  <chr>                       <chr> <chr>                               <chr>   \n1 Belgian Waffles             $5.95 Two of our famous Belgian Waffles … 650     \n2 Strawberry Belgian Waffles  $7.95 Light Belgian waffles covered with… 900     \n3 Berry-Berry Belgian Waffles $8.95 Light Belgian waffles covered with… 900     \n4 French Toast                $4.50 Thick slices made from our homemad… 600     \n5 Homestyle Breakfast         $6.95 Two eggs, bacon or sausage, toast,… 950     \n\n\n\n\n\nxml_data\n\n                          name  ... calories\n0              Belgian Waffles  ...      650\n1   Strawberry Belgian Waffles  ...      900\n2  Berry-Berry Belgian Waffles  ...      900\n3                 French Toast  ...      600\n4          Homestyle Breakfast  ...      950\n\n[5 rows x 4 columns]"
  },
  {
    "objectID": "chapters/dc_unstructured.html",
    "href": "chapters/dc_unstructured.html",
    "title": "4  Unstructured Data",
    "section": "",
    "text": "RPython\n\n\n\nlibrary(jsonlite)\nlibrary(xml2)\n\n\n\n\nimport pandas as pd"
  },
  {
    "objectID": "chapters/dc_unstructured.html#html-pages",
    "href": "chapters/dc_unstructured.html#html-pages",
    "title": "4  Unstructured Data",
    "section": "4.1 HTML pages",
    "text": "4.1 HTML pages"
  },
  {
    "objectID": "chapters/dc_unstructured.html#pdfs",
    "href": "chapters/dc_unstructured.html#pdfs",
    "title": "4  Unstructured Data",
    "section": "4.2 PDFs",
    "text": "4.2 PDFs"
  },
  {
    "objectID": "chapters/p02_dm.html",
    "href": "chapters/p02_dm.html",
    "title": "Data Manipulation",
    "section": "",
    "text": "R has an ecosystem of packages, the tidyverse, each specialized in a single task:\n\nreadr for importing data from various sources;\ntidyr and dplyr for data wrangling;\nggplot2 for plotting;\nand others.\n\nIn Python, it is possible to accomplish almost every data manipulation task with the Pandas package."
  },
  {
    "objectID": "chapters/dm_subset.html",
    "href": "chapters/dm_subset.html",
    "title": "5  Subset Data",
    "section": "",
    "text": "By subsetting data, it is intended all those operations used to retrieve a specific part of a dataset. For example, we could be interested in getting the last three rows of a table or the columns whose name starts with a specific keyword.\nFor this chapter, we will consider the Personal key indicators of heart disease Kaggle dataset.\nAfter loading the necessary packages and importing the data, we make use of the clean_names function of the janitor and pyjanitor packages. This function can be used to make lowercase and snake case the dataset column names."
  },
  {
    "objectID": "chapters/dm_subset.html#filter-rows",
    "href": "chapters/dm_subset.html#filter-rows",
    "title": "5  Subset Data",
    "section": "5.1 Filter rows",
    "text": "5.1 Filter rows\n\n5.1.1 Top\nGet the first three rows of a dataset:\n\nRPython\n\n\n\nhead(df, n = 3)\n\n# A tibble: 3 × 18\n  heart_disease   bmi smoking alcohol_drinking stroke physical_health\n  <chr>         <dbl> <chr>   <chr>            <chr>            <dbl>\n1 No             16.6 Yes     No               No                   3\n2 No             20.3 No      No               Yes                  0\n3 No             26.6 Yes     No               No                  20\n# … with 12 more variables: mental_health <dbl>, diff_walking <chr>, sex <chr>,\n#   age_category <chr>, race <chr>, diabetic <chr>, physical_activity <chr>,\n#   gen_health <chr>, sleep_time <dbl>, asthma <chr>, kidney_disease <chr>,\n#   skin_cancer <chr>\n\n\n\n\nSynonyms\ndf |> slice_head(n = 3)\n\n\n\n\n\ndf.head(n=3)\n\n  heart_disease    bmi smoking  ... asthma kidney_disease  skin_cancer\n0            No  16.60     Yes  ...    Yes             No          Yes\n1            No  20.34      No  ...     No             No           No\n2            No  26.58     Yes  ...    Yes             No           No\n\n[3 rows x 18 columns]\n\n\n\n\n\n\n\n5.1.2 Bottom\nGet the last five rows of a dataset:\n\nRPython\n\n\n\ntail(df, n = 5)\n\n# A tibble: 5 × 18\n  heart_disease   bmi smoking alcohol_drinking stroke physical_health\n  <chr>         <dbl> <chr>   <chr>            <chr>            <dbl>\n1 Yes            27.4 Yes     No               No                   7\n2 No             29.8 Yes     No               No                   0\n3 No             24.2 No      No               No                   0\n4 No             32.8 No      No               No                   0\n5 No             46.6 No      No               No                   0\n# … with 12 more variables: mental_health <dbl>, diff_walking <chr>, sex <chr>,\n#   age_category <chr>, race <chr>, diabetic <chr>, physical_activity <chr>,\n#   gen_health <chr>, sleep_time <dbl>, asthma <chr>, kidney_disease <chr>,\n#   skin_cancer <chr>\n\n\n\n\nSynonyms\ndf |> slice_tail(n = 5)\n\n\n\n\n\ndf.tail(n=5)\n\n       heart_disease    bmi smoking  ... asthma kidney_disease  skin_cancer\n319790           Yes  27.41     Yes  ...    Yes             No           No\n319791            No  29.84     Yes  ...    Yes             No           No\n319792            No  24.24      No  ...     No             No           No\n319793            No  32.81      No  ...     No             No           No\n319794            No  46.56      No  ...     No             No           No\n\n[5 rows x 18 columns]\n\n\n\n\n\n\n\n5.1.3 Slicing\nGet rows number seven to eleven:\n\nRPython\n\n\n\ndf[7:11, ]\n\n# A tibble: 5 × 18\n  heart_disease   bmi smoking alcohol_drinking stroke physical_health\n  <chr>         <dbl> <chr>   <chr>            <chr>            <dbl>\n1 No             21.6 No      No               No                  15\n2 No             31.6 Yes     No               No                   5\n3 No             26.4 No      No               No                   0\n4 No             40.7 No      No               No                   0\n5 Yes            34.3 Yes     No               No                  30\n# … with 12 more variables: mental_health <dbl>, diff_walking <chr>, sex <chr>,\n#   age_category <chr>, race <chr>, diabetic <chr>, physical_activity <chr>,\n#   gen_health <chr>, sleep_time <dbl>, asthma <chr>, kidney_disease <chr>,\n#   skin_cancer <chr>\n\n\n\n\nSynonyms\ndf |> slice(7:11)\n\n\n\n\n\ndf.iloc[6:11,]\n\n   heart_disease    bmi smoking  ... asthma kidney_disease  skin_cancer\n6             No  21.63      No  ...    Yes             No          Yes\n7             No  31.64     Yes  ...    Yes             No           No\n8             No  26.45      No  ...     No            Yes           No\n9             No  40.69      No  ...     No             No           No\n10           Yes  34.30     Yes  ...    Yes             No           No\n\n[5 rows x 18 columns]\n\n\n\n\n\n\n\n5.1.4 By column values\nFilter rows by column values:\n\nRPython\n\n\n\ndf[(df$bmi < 20) & (df$smoking == \"Yes\"), ]\n\n# A tibble: 6,061 × 18\n   heart_disease   bmi smoking alcohol_drinking stroke physical_health\n   <chr>         <dbl> <chr>   <chr>            <chr>            <dbl>\n 1 No             16.6 Yes     No               No                   3\n 2 No             19.0 Yes     No               No                   0\n 3 No             19.5 Yes     No               No                   0\n 4 No             19.3 Yes     No               No                   0\n 5 No             18.8 Yes     No               Yes                  0\n 6 No             18.8 Yes     Yes              Yes                 30\n 7 No             17.6 Yes     No               No                   2\n 8 No             19.1 Yes     No               No                   0\n 9 No             14.7 Yes     No               No                   0\n10 No             19.0 Yes     No               No                  10\n# … with 6,051 more rows, and 12 more variables: mental_health <dbl>,\n#   diff_walking <chr>, sex <chr>, age_category <chr>, race <chr>,\n#   diabetic <chr>, physical_activity <chr>, gen_health <chr>,\n#   sleep_time <dbl>, asthma <chr>, kidney_disease <chr>, skin_cancer <chr>\n\n\n\n\nSynonyms\ndf |> filter(bmi < 20, smoking == \"Yes\")\n\n\n\n\n\ndf.loc[(df[\"bmi\"] < 20) & (df[\"smoking\"] == \"Yes\"),:]\n\n       heart_disease    bmi smoking  ... asthma kidney_disease  skin_cancer\n0                 No  16.60     Yes  ...    Yes             No          Yes\n33                No  19.02     Yes  ...     No             No           No\n99                No  19.47     Yes  ...     No             No           No\n189               No  19.31     Yes  ...     No             No           No\n250               No  18.84     Yes  ...     No             No           No\n...              ...    ...     ...  ...    ...            ...          ...\n318688            No  19.20     Yes  ...     No             No           No\n319455            No  19.77     Yes  ...     No             No           No\n319602            No  17.94     Yes  ...     No             No           No\n319678            No  18.42     Yes  ...    Yes             No           No\n319712            No  17.38     Yes  ...     No             No           No\n\n[6061 rows x 18 columns]\n\n\n\n\nSynonyms\ndf.query('bmi < 20 & smoking == \"Yes\"')"
  },
  {
    "objectID": "chapters/dm_subset.html#select-two-or-more-columns",
    "href": "chapters/dm_subset.html#select-two-or-more-columns",
    "title": "5  Subset Data",
    "section": "5.2 Select two or more columns",
    "text": "5.2 Select two or more columns\n\n5.2.1 By position\nSelect three columns by their position:\n\nRPython\n\n\n\ndf[1:3]\n\n# A tibble: 319,795 × 3\n   heart_disease   bmi smoking\n   <chr>         <dbl> <chr>  \n 1 No             16.6 Yes    \n 2 No             20.3 No     \n 3 No             26.6 Yes    \n 4 No             24.2 No     \n 5 No             23.7 No     \n 6 Yes            28.9 Yes    \n 7 No             21.6 No     \n 8 No             31.6 Yes    \n 9 No             26.4 No     \n10 No             40.7 No     \n# … with 319,785 more rows\n\n\n\n\nSynonyms\ndf |> select(1:3)\n\n\n\n\n\ndf.iloc[:,0:3]\n\n       heart_disease    bmi smoking\n0                 No  16.60     Yes\n1                 No  20.34      No\n2                 No  26.58     Yes\n3                 No  24.21      No\n4                 No  23.71      No\n...              ...    ...     ...\n319790           Yes  27.41     Yes\n319791            No  29.84     Yes\n319792            No  24.24      No\n319793            No  32.81      No\n319794            No  46.56      No\n\n[319795 rows x 3 columns]\n\n\n\n\n\n\n\n5.2.2 By name\nSelect three columns by their name:\n\nRPython\n\n\n\ndf[c(\"heart_disease\", \"bmi\", \"smoking\")]\n\n# A tibble: 319,795 × 3\n   heart_disease   bmi smoking\n   <chr>         <dbl> <chr>  \n 1 No             16.6 Yes    \n 2 No             20.3 No     \n 3 No             26.6 Yes    \n 4 No             24.2 No     \n 5 No             23.7 No     \n 6 Yes            28.9 Yes    \n 7 No             21.6 No     \n 8 No             31.6 Yes    \n 9 No             26.4 No     \n10 No             40.7 No     \n# … with 319,785 more rows\n\n\n\n\nSynonyms\ndf |> select(heart_disease, bmi, smoking)\ndf |> select(heart_disease:smoking)\n\n\n\n\n\ndf.loc[:, [\"heart_disease\", \"bmi\", \"smoking\"]]\n\n       heart_disease    bmi smoking\n0                 No  16.60     Yes\n1                 No  20.34      No\n2                 No  26.58     Yes\n3                 No  24.21      No\n4                 No  23.71      No\n...              ...    ...     ...\n319790           Yes  27.41     Yes\n319791            No  29.84     Yes\n319792            No  24.24      No\n319793            No  32.81      No\n319794            No  46.56      No\n\n[319795 rows x 3 columns]\n\n\n\n\nSynonyms\ndf.loc[:, \"heart_disease\":\"smoking\"]\n\n\n\n\n\n\n\n5.2.3 By pattern\nSelect columns by a regular expression or pattern in their name:\n\nRPython\n\n\n\ndf |> select(ends_with(\"health\"))\n\n# A tibble: 319,795 × 3\n   physical_health mental_health gen_health\n             <dbl>         <dbl> <chr>     \n 1               3            30 Very good \n 2               0             0 Very good \n 3              20            30 Fair      \n 4               0             0 Good      \n 5              28             0 Very good \n 6               6             0 Fair      \n 7              15             0 Fair      \n 8               5             0 Good      \n 9               0             0 Fair      \n10               0             0 Good      \n# … with 319,785 more rows\n\n\n\n\n\ndf.filter(like=\"health\", axis=1)\n\n        physical_health  mental_health gen_health\n0                   3.0           30.0  Very good\n1                   0.0            0.0  Very good\n2                  20.0           30.0       Fair\n3                   0.0            0.0       Good\n4                  28.0            0.0  Very good\n...                 ...            ...        ...\n319790              7.0            0.0       Fair\n319791              0.0            0.0  Very good\n319792              0.0            0.0       Good\n319793              0.0            0.0       Good\n319794              0.0            0.0       Good\n\n[319795 rows x 3 columns]\n\n\n\n\n\n\n\n5.2.4 By type\nSelect columns based on their data type:\n\nRPython\n\n\n\ndf |> select(where(is.double))\n\n# A tibble: 319,795 × 4\n     bmi physical_health mental_health sleep_time\n   <dbl>           <dbl>         <dbl>      <dbl>\n 1  16.6               3            30          5\n 2  20.3               0             0          7\n 3  26.6              20            30          8\n 4  24.2               0             0          6\n 5  23.7              28             0          8\n 6  28.9               6             0         12\n 7  21.6              15             0          4\n 8  31.6               5             0          9\n 9  26.4               0             0          5\n10  40.7               0             0         10\n# … with 319,785 more rows\n\n\n\n\n\ndf.select_dtypes(float)\n\n          bmi  physical_health  mental_health  sleep_time\n0       16.60              3.0           30.0         5.0\n1       20.34              0.0            0.0         7.0\n2       26.58             20.0           30.0         8.0\n3       24.21              0.0            0.0         6.0\n4       23.71             28.0            0.0         8.0\n...       ...              ...            ...         ...\n319790  27.41              7.0            0.0         6.0\n319791  29.84              0.0            0.0         5.0\n319792  24.24              0.0            0.0         6.0\n319793  32.81              0.0            0.0        12.0\n319794  46.56              0.0            0.0         8.0\n\n[319795 rows x 4 columns]"
  },
  {
    "objectID": "chapters/dm_subset.html#remove-columns",
    "href": "chapters/dm_subset.html#remove-columns",
    "title": "5  Subset Data",
    "section": "5.3 Remove columns",
    "text": "5.3 Remove columns\nRemove one column:\n\nRPython\n\n\n\ndf |> select(-bmi)\n\n# A tibble: 319,795 × 17\n   heart_disease smoking alcohol_drinking stroke physical_health mental_health\n   <chr>         <chr>   <chr>            <chr>            <dbl>         <dbl>\n 1 No            Yes     No               No                   3            30\n 2 No            No      No               Yes                  0             0\n 3 No            Yes     No               No                  20            30\n 4 No            No      No               No                   0             0\n 5 No            No      No               No                  28             0\n 6 Yes           Yes     No               No                   6             0\n 7 No            No      No               No                  15             0\n 8 No            Yes     No               No                   5             0\n 9 No            No      No               No                   0             0\n10 No            No      No               No                   0             0\n# … with 319,785 more rows, and 11 more variables: diff_walking <chr>,\n#   sex <chr>, age_category <chr>, race <chr>, diabetic <chr>,\n#   physical_activity <chr>, gen_health <chr>, sleep_time <dbl>, asthma <chr>,\n#   kidney_disease <chr>, skin_cancer <chr>\n\n\n\n\n\ndf.drop(\"bmi\", axis=1)\n\n       heart_disease smoking alcohol_drinking  ... asthma  kidney_disease  skin_cancer\n0                 No     Yes               No  ...    Yes              No          Yes\n1                 No      No               No  ...     No              No           No\n2                 No     Yes               No  ...    Yes              No           No\n3                 No      No               No  ...     No              No          Yes\n4                 No      No               No  ...     No              No           No\n...              ...     ...              ...  ...    ...             ...          ...\n319790           Yes     Yes               No  ...    Yes              No           No\n319791            No     Yes               No  ...    Yes              No           No\n319792            No      No               No  ...     No              No           No\n319793            No      No               No  ...     No              No           No\n319794            No      No               No  ...     No              No           No\n\n[319795 rows x 17 columns]\n\n\n\n\n\nRemove more than one column:\n\nRPython\n\n\n\ndf |> select(-c(bmi, heart_disease))\n\n# A tibble: 319,795 × 16\n   smoking alcohol_drinking stroke physical_health mental_health diff_walking\n   <chr>   <chr>            <chr>            <dbl>         <dbl> <chr>       \n 1 Yes     No               No                   3            30 No          \n 2 No      No               Yes                  0             0 No          \n 3 Yes     No               No                  20            30 No          \n 4 No      No               No                   0             0 No          \n 5 No      No               No                  28             0 Yes         \n 6 Yes     No               No                   6             0 Yes         \n 7 No      No               No                  15             0 No          \n 8 Yes     No               No                   5             0 Yes         \n 9 No      No               No                   0             0 No          \n10 No      No               No                   0             0 Yes         \n# … with 319,785 more rows, and 10 more variables: sex <chr>,\n#   age_category <chr>, race <chr>, diabetic <chr>, physical_activity <chr>,\n#   gen_health <chr>, sleep_time <dbl>, asthma <chr>, kidney_disease <chr>,\n#   skin_cancer <chr>\n\n\n\n\n\ndf.drop([\"bmi\", \"heart_disease\"], axis=1)\n\n       smoking alcohol_drinking stroke  ...  asthma  kidney_disease skin_cancer\n0          Yes               No     No  ...     Yes              No         Yes\n1           No               No    Yes  ...      No              No          No\n2          Yes               No     No  ...     Yes              No          No\n3           No               No     No  ...      No              No         Yes\n4           No               No     No  ...      No              No          No\n...        ...              ...    ...  ...     ...             ...         ...\n319790     Yes               No     No  ...     Yes              No          No\n319791     Yes               No     No  ...     Yes              No          No\n319792      No               No     No  ...      No              No          No\n319793      No               No     No  ...      No              No          No\n319794      No               No     No  ...      No              No          No\n\n[319795 rows x 16 columns]"
  },
  {
    "objectID": "chapters/dm_subset.html#sorting",
    "href": "chapters/dm_subset.html#sorting",
    "title": "5  Subset Data",
    "section": "5.4 Sorting",
    "text": "5.4 Sorting\n\n5.4.1 Ascending\nSort a dataset by column values in ascending order:\n\nRPython\n\n\n\ndf |> arrange(sex)\n\n# A tibble: 319,795 × 18\n   heart_disease   bmi smoking alcohol_drinking stroke physical_health\n   <chr>         <dbl> <chr>   <chr>            <chr>            <dbl>\n 1 No             16.6 Yes     No               No                   3\n 2 No             20.3 No      No               Yes                  0\n 3 No             24.2 No      No               No                   0\n 4 No             23.7 No      No               No                  28\n 5 Yes            28.9 Yes     No               No                   6\n 6 No             21.6 No      No               No                  15\n 7 No             31.6 Yes     No               No                   5\n 8 No             26.4 No      No               No                   0\n 9 No             28.7 Yes     No               No                   0\n10 No             28.2 No      No               No                   7\n# … with 319,785 more rows, and 12 more variables: mental_health <dbl>,\n#   diff_walking <chr>, sex <chr>, age_category <chr>, race <chr>,\n#   diabetic <chr>, physical_activity <chr>, gen_health <chr>,\n#   sleep_time <dbl>, asthma <chr>, kidney_disease <chr>, skin_cancer <chr>\n\n\n\n\n\ndf.sort_values(\"sex\")\n\n       heart_disease    bmi smoking  ... asthma kidney_disease  skin_cancer\n0                 No  16.60     Yes  ...    Yes             No          Yes\n179722            No  29.29      No  ...    Yes             No           No\n179725           Yes  36.58     Yes  ...     No             No          Yes\n179728            No  24.96      No  ...     No             No           No\n179730            No  25.06     Yes  ...     No             No           No\n...              ...    ...     ...  ...    ...            ...          ...\n139706            No  42.91     Yes  ...     No             No           No\n94088            Yes  38.39     Yes  ...     No             No           No\n212147            No  37.59      No  ...     No             No           No\n56348             No  32.89     Yes  ...    Yes             No          Yes\n109843            No  37.03     Yes  ...     No             No           No\n\n[319795 rows x 18 columns]\n\n\n\n\n\n\n\n5.4.2 Descending\nSort a dataset by column values in descending order:\n\nRPython\n\n\n\ndf |> arrange(-bmi)\n\n# A tibble: 319,795 × 18\n   heart_disease   bmi smoking alcohol_drinking stroke physical_health\n   <chr>         <dbl> <chr>   <chr>            <chr>            <dbl>\n 1 No             94.8 No      No               No                   0\n 2 No             94.7 No      No               No                   4\n 3 No             94.0 Yes     No               No                  20\n 4 No             93.9 Yes     Yes              No                  30\n 5 No             92.5 Yes     No               No                   7\n 6 No             91.8 No      No               No                   0\n 7 No             91.6 Yes     No               No                   0\n 8 No             91.6 No      No               No                   0\n 9 No             88.6 No      No               No                  30\n10 No             88.2 No      No               No                   0\n# … with 319,785 more rows, and 12 more variables: mental_health <dbl>,\n#   diff_walking <chr>, sex <chr>, age_category <chr>, race <chr>,\n#   diabetic <chr>, physical_activity <chr>, gen_health <chr>,\n#   sleep_time <dbl>, asthma <chr>, kidney_disease <chr>, skin_cancer <chr>\n\n\n\n\n\ndf.sort_values(\"bmi\", ascending=False)\n\n       heart_disease    bmi smoking  ... asthma kidney_disease  skin_cancer\n126896            No  94.85      No  ...     No             No           No\n242834            No  94.66      No  ...     No             No           No\n104267            No  93.97     Yes  ...     No             No           No\n249715            No  93.86     Yes  ...    Yes            Yes           No\n156093            No  92.53     Yes  ...    Yes             No           No\n...              ...    ...     ...  ...    ...            ...          ...\n81754             No  12.16      No  ...     No             No           No\n51637             No  12.13      No  ...     No             No           No\n113373            No  12.08     Yes  ...     No             No           No\n69662             No  12.02     Yes  ...     No             No           No\n205511            No  12.02      No  ...     No             No           No\n\n[319795 rows x 18 columns]"
  },
  {
    "objectID": "chapters/dm_sum_mod.html#modify-data",
    "href": "chapters/dm_sum_mod.html#modify-data",
    "title": "6  Modify, Group, Summarize",
    "section": "6.1 Modify data",
    "text": "6.1 Modify data\n\nRPython\n\n\n\ncols_to_mod <- c(\"smoking\", \"asthma\", \"skin_cancer\")\ndf |> mutate(across(all_of(cols_to_mod), stringr::str_to_lower))\n\n# A tibble: 319,795 × 18\n   heart_disease   bmi smoking alcohol_drinking stroke physical_health\n   <chr>         <dbl> <chr>   <chr>            <chr>            <dbl>\n 1 No             16.6 yes     No               No                   3\n 2 No             20.3 no      No               Yes                  0\n 3 No             26.6 yes     No               No                  20\n 4 No             24.2 no      No               No                   0\n 5 No             23.7 no      No               No                  28\n 6 Yes            28.9 yes     No               No                   6\n 7 No             21.6 no      No               No                  15\n 8 No             31.6 yes     No               No                   5\n 9 No             26.4 no      No               No                   0\n10 No             40.7 no      No               No                   0\n# … with 319,785 more rows, and 12 more variables: mental_health <dbl>,\n#   diff_walking <chr>, sex <chr>, age_category <chr>, race <chr>,\n#   diabetic <chr>, physical_activity <chr>, gen_health <chr>,\n#   sleep_time <dbl>, asthma <chr>, kidney_disease <chr>, skin_cancer <chr>\n\n\n\ndf <- df |> mutate(bmi_class = case_when(\n    bmi < 18.5 ~ \"underweight\",\n    bmi >= 18.5 & bmi < 25 ~ \"normal weight\",\n    bmi >= 25 & bmi < 30 ~ \"overweight\",\n    bmi >= 30 ~ \"obese\",\n    TRUE ~ NA_character_\n))\n\n\n\n\ncols_to_mod = [\"smoking\", \"asthma\", \"skin_cancer\"]\ndf.loc[:,cols_to_mod].applymap(str.lower)\n\n       smoking asthma skin_cancer\n0          yes    yes         yes\n1           no     no          no\n2          yes    yes          no\n3           no     no         yes\n4           no     no          no\n...        ...    ...         ...\n319790     yes    yes          no\n319791     yes    yes          no\n319792      no     no          no\n319793      no     no          no\n319794      no     no          no\n\n[319795 rows x 3 columns]\n\n\n\ndf = df.assign(bmi_class=pd.cut(df[\"bmi\"], \n    bins=[0, 18.5, 25, 30, 1000], \n    labels=[\"underweight\", \"normal weight\", \"overweight\", \"obese\"],\n    ordered=True))\n\n# df[\"bmi_class\"] = pd.cut(df[\"bmi\"], \n#     bins=[0, 18.5, 25, 30, 1000], \n#     labels=[\"underweight\", \"normal weight\", \"overweight\", \"obese\"],\n#     ordered=True)"
  },
  {
    "objectID": "chapters/dm_sum_mod.html#count-values",
    "href": "chapters/dm_sum_mod.html#count-values",
    "title": "6  Modify, Group, Summarize",
    "section": "6.2 Count values",
    "text": "6.2 Count values\n\nRPython\n\n\n\ndf |> count(smoking, asthma)\n\n# A tibble: 4 × 3\n  smoking asthma      n\n  <chr>   <chr>   <int>\n1 No      No     163994\n2 No      Yes     23893\n3 Yes     No     112929\n4 Yes     Yes     18979\n\n\n\n\n\ndf.value_counts([\"smoking\", \"asthma\"])\n\nsmoking  asthma\nNo       No        163994\nYes      No        112929\nNo       Yes        23893\nYes      Yes        18979\ndtype: int64"
  },
  {
    "objectID": "chapters/dm_sum_mod.html#cross-tabulation",
    "href": "chapters/dm_sum_mod.html#cross-tabulation",
    "title": "6  Modify, Group, Summarize",
    "section": "6.3 Cross-tabulation",
    "text": "6.3 Cross-tabulation\n\nRPython\n\n\n\ndf |> select(smoking, asthma) |> table()\n\n       asthma\nsmoking     No    Yes\n    No  163994  23893\n    Yes 112929  18979\n\n\n\n\n\npd.crosstab(df.smoking, df.asthma)\n\nasthma       No    Yes\nsmoking               \nNo       163994  23893\nYes      112929  18979"
  },
  {
    "objectID": "chapters/dm_sum_mod.html#grouping",
    "href": "chapters/dm_sum_mod.html#grouping",
    "title": "6  Modify, Group, Summarize",
    "section": "6.4 Grouping",
    "text": "6.4 Grouping\n\nRPython\n\n\n\ndf |> group_by(heart_disease)\n\n# A tibble: 319,795 × 19\n# Groups:   heart_disease [2]\n   heart_disease   bmi smoking alcohol_drinking stroke physical_health\n   <chr>         <dbl> <chr>   <chr>            <chr>            <dbl>\n 1 No             16.6 Yes     No               No                   3\n 2 No             20.3 No      No               Yes                  0\n 3 No             26.6 Yes     No               No                  20\n 4 No             24.2 No      No               No                   0\n 5 No             23.7 No      No               No                  28\n 6 Yes            28.9 Yes     No               No                   6\n 7 No             21.6 No      No               No                  15\n 8 No             31.6 Yes     No               No                   5\n 9 No             26.4 No      No               No                   0\n10 No             40.7 No      No               No                   0\n# … with 319,785 more rows, and 13 more variables: mental_health <dbl>,\n#   diff_walking <chr>, sex <chr>, age_category <chr>, race <chr>,\n#   diabetic <chr>, physical_activity <chr>, gen_health <chr>,\n#   sleep_time <dbl>, asthma <chr>, kidney_disease <chr>, skin_cancer <chr>,\n#   bmi_class <chr>\n\n\n\n\n\ndf.groupby(\"heart_disease\")\n\n<pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f5935a4bfd0>"
  },
  {
    "objectID": "chapters/dm_sum_mod.html#summarize-data",
    "href": "chapters/dm_sum_mod.html#summarize-data",
    "title": "6  Modify, Group, Summarize",
    "section": "6.5 Summarize data",
    "text": "6.5 Summarize data\n\nRPython\n\n\n\ndf |> \n    group_by(heart_disease) |> \n    summarize(min = min(bmi), mean = mean(bmi), max = max(bmi))\n\n# A tibble: 2 × 4\n  heart_disease   min  mean   max\n  <chr>         <dbl> <dbl> <dbl>\n1 No             12.0  28.2  94.8\n2 Yes            12.2  29.4  83.3\n\n\n\n\n\n(\n    df\n    .groupby(\"heart_disease\")\n    .agg({'bmi': ['min', 'mean', 'max']})\n)\n\n                 bmi                  \n                 min       mean    max\nheart_disease                         \nNo             12.02  28.224658  94.85\nYes            12.21  29.401592  83.33"
  },
  {
    "objectID": "chapters/dm_pivot_join.html",
    "href": "chapters/dm_pivot_join.html",
    "title": "7  Pivoting and Joining",
    "section": "",
    "text": "After importing the data as in ?sec-import_local, we must load the necessary packages:"
  },
  {
    "objectID": "chapters/dm_pivot_join.html#pivot",
    "href": "chapters/dm_pivot_join.html#pivot",
    "title": "7  Pivoting and Joining",
    "section": "7.1 Pivot",
    "text": "7.1 Pivot\nFirst, we select the first ten rows of our dataframe and the columns heart_disease, stroke, smoking and bmi.\n\nRPython\n\n\n\ndf3 <- df[1:10, ] |> \n    select(heart_disease, stroke, smoking, bmi) |> \n    mutate(patient_id = stringr::str_c(\"P\", 1:10), .before = 1)\ndf3\n\n# A tibble: 10 × 5\n   patient_id heart_disease stroke smoking   bmi\n   <chr>      <chr>         <chr>  <chr>   <dbl>\n 1 P1         No            No     Yes      16.6\n 2 P2         No            Yes    No       20.3\n 3 P3         No            No     Yes      26.6\n 4 P4         No            No     No       24.2\n 5 P5         No            No     No       23.7\n 6 P6         Yes           No     Yes      28.9\n 7 P7         No            No     No       21.6\n 8 P8         No            No     Yes      31.6\n 9 P9         No            No     No       26.4\n10 P10        No            No     No       40.7\n\n\n\n\n\ndf3 = df.loc[0:9, [\"heart_disease\", \"stroke\", \"smoking\", \"bmi\"]]\ndf3.insert(0, \"patient_id\", [\"P\" + str(i) for i in range(1, 11)])\ndf3\n\n  patient_id heart_disease stroke smoking    bmi\n0         P1            No     No     Yes  16.60\n1         P2            No    Yes      No  20.34\n2         P3            No     No     Yes  26.58\n3         P4            No     No      No  24.21\n4         P5            No     No      No  23.71\n5         P6           Yes     No     Yes  28.87\n6         P7            No     No      No  21.63\n7         P8            No     No     Yes  31.64\n8         P9            No     No      No  26.45\n9        P10            No     No      No  40.69\n\n\n\n\n\n\n7.1.1 Long format\n\nRPython\n\n\n\ndf4 <- df3 |> \n    pivot_longer(-patient_id, names_to = \"variable\", values_to = \"value\",\n                 values_transform = as.character)\ndf4\n\n# A tibble: 40 × 3\n   patient_id variable      value\n   <chr>      <chr>         <chr>\n 1 P1         heart_disease No   \n 2 P1         stroke        No   \n 3 P1         smoking       Yes  \n 4 P1         bmi           16.6 \n 5 P2         heart_disease No   \n 6 P2         stroke        Yes  \n 7 P2         smoking       No   \n 8 P2         bmi           20.34\n 9 P3         heart_disease No   \n10 P3         stroke        No   \n# … with 30 more rows\n\n\n\n\n\ndf4 = df3.melt(id_vars=\"patient_id\", var_name=\"variable\", value_name=\"value\")\ndf4\n\n   patient_id       variable  value\n0          P1  heart_disease     No\n1          P2  heart_disease     No\n2          P3  heart_disease     No\n3          P4  heart_disease     No\n4          P5  heart_disease     No\n5          P6  heart_disease    Yes\n6          P7  heart_disease     No\n7          P8  heart_disease     No\n8          P9  heart_disease     No\n9         P10  heart_disease     No\n10         P1         stroke     No\n11         P2         stroke    Yes\n12         P3         stroke     No\n13         P4         stroke     No\n14         P5         stroke     No\n15         P6         stroke     No\n16         P7         stroke     No\n17         P8         stroke     No\n18         P9         stroke     No\n19        P10         stroke     No\n20         P1        smoking    Yes\n21         P2        smoking     No\n22         P3        smoking    Yes\n23         P4        smoking     No\n24         P5        smoking     No\n25         P6        smoking    Yes\n26         P7        smoking     No\n27         P8        smoking    Yes\n28         P9        smoking     No\n29        P10        smoking     No\n30         P1            bmi   16.6\n31         P2            bmi  20.34\n32         P3            bmi  26.58\n33         P4            bmi  24.21\n34         P5            bmi  23.71\n35         P6            bmi  28.87\n36         P7            bmi  21.63\n37         P8            bmi  31.64\n38         P9            bmi  26.45\n39        P10            bmi  40.69\n\n\n\n\n\n\n\n7.1.2 Wide format\n\nRPython\n\n\n\ndf4 |> pivot_wider(patient_id, names_from = variable, values_from = value)\n\n# A tibble: 10 × 5\n   patient_id heart_disease stroke smoking bmi  \n   <chr>      <chr>         <chr>  <chr>   <chr>\n 1 P1         No            No     Yes     16.6 \n 2 P2         No            Yes    No      20.34\n 3 P3         No            No     Yes     26.58\n 4 P4         No            No     No      24.21\n 5 P5         No            No     No      23.71\n 6 P6         Yes           No     Yes     28.87\n 7 P7         No            No     No      21.63\n 8 P8         No            No     Yes     31.64\n 9 P9         No            No     No      26.45\n10 P10        No            No     No      40.69\n\n\n\n\n\ndf4.pivot(index=\"patient_id\", columns=\"variable\", values=\"value\")\n\nvariable      bmi heart_disease smoking stroke\npatient_id                                    \nP1           16.6            No     Yes     No\nP10         40.69            No      No     No\nP2          20.34            No      No    Yes\nP3          26.58            No     Yes     No\nP4          24.21            No      No     No\nP5          23.71            No      No     No\nP6          28.87           Yes     Yes     No\nP7          21.63            No      No     No\nP8          31.64            No     Yes     No\nP9          26.45            No      No     No"
  },
  {
    "objectID": "chapters/dm_pivot_join.html#merge",
    "href": "chapters/dm_pivot_join.html#merge",
    "title": "7  Pivoting and Joining",
    "section": "7.2 Merge",
    "text": "7.2 Merge\n\nRPython\n\n\n\nset.seed(123)\ndf2 <- tibble(\n    gen_health = c(\"Poor\", \"Fair\", \"Good\", \"Very good\", \"Excellent\"),\n    random_biomarker = rnorm(5)\n)\ndf2\n\n# A tibble: 5 × 2\n  gen_health random_biomarker\n  <chr>                 <dbl>\n1 Poor                -0.560 \n2 Fair                -0.230 \n3 Good                 1.56  \n4 Very good            0.0705\n5 Excellent            0.129 \n\n\n\n\n\nimport numpy as np\n\nrng = np.random.RandomState(123)\ndf2 = pd.DataFrame({\n    \"gen_health\": [\"Poor\", \"Fair\", \"Good\", \"Very good\", \"Excellent\"],\n    \"random_biomarker\": rng.randn(5)\n})\ndf2\n\n  gen_health  random_biomarker\n0       Poor         -1.085631\n1       Fair          0.997345\n2       Good          0.282978\n3  Very good         -1.506295\n4  Excellent         -0.578600\n\n\n\n\n\n\n7.2.1 Left-join\n\nRPython\n\n\n\ndf |> left_join(df2, by = \"gen_health\")\n\n# A tibble: 319,795 × 19\n   heart_disease   bmi smoking alcohol_drinking stroke physical_health\n   <chr>         <dbl> <chr>   <chr>            <chr>            <dbl>\n 1 No             16.6 Yes     No               No                   3\n 2 No             20.3 No      No               Yes                  0\n 3 No             26.6 Yes     No               No                  20\n 4 No             24.2 No      No               No                   0\n 5 No             23.7 No      No               No                  28\n 6 Yes            28.9 Yes     No               No                   6\n 7 No             21.6 No      No               No                  15\n 8 No             31.6 Yes     No               No                   5\n 9 No             26.4 No      No               No                   0\n10 No             40.7 No      No               No                   0\n# … with 319,785 more rows, and 13 more variables: mental_health <dbl>,\n#   diff_walking <chr>, sex <chr>, age_category <chr>, race <chr>,\n#   diabetic <chr>, physical_activity <chr>, gen_health <chr>,\n#   sleep_time <dbl>, asthma <chr>, kidney_disease <chr>, skin_cancer <chr>,\n#   random_biomarker <dbl>\n\n\n\n\n\ndf.join(df2.set_index(\"gen_health\"), on='gen_health', how=\"left\")\n\n       heart_disease    bmi  ... skin_cancer random_biomarker\n0                 No  16.60  ...         Yes        -1.506295\n1                 No  20.34  ...          No        -1.506295\n2                 No  26.58  ...          No         0.997345\n3                 No  24.21  ...         Yes         0.282978\n4                 No  23.71  ...          No        -1.506295\n...              ...    ...  ...         ...              ...\n319790           Yes  27.41  ...          No         0.997345\n319791            No  29.84  ...          No        -1.506295\n319792            No  24.24  ...          No         0.282978\n319793            No  32.81  ...          No         0.282978\n319794            No  46.56  ...          No         0.282978\n\n[319795 rows x 19 columns]\n\n\n\n\n\n\n\n7.2.2 Right-join\n\nRPython\n\n\n\ndf |> right_join(df2, by = \"gen_health\")\n\n# A tibble: 319,795 × 19\n   heart_disease   bmi smoking alcohol_drinking stroke physical_health\n   <chr>         <dbl> <chr>   <chr>            <chr>            <dbl>\n 1 No             16.6 Yes     No               No                   3\n 2 No             20.3 No      No               Yes                  0\n 3 No             26.6 Yes     No               No                  20\n 4 No             24.2 No      No               No                   0\n 5 No             23.7 No      No               No                  28\n 6 Yes            28.9 Yes     No               No                   6\n 7 No             21.6 No      No               No                  15\n 8 No             31.6 Yes     No               No                   5\n 9 No             26.4 No      No               No                   0\n10 No             40.7 No      No               No                   0\n# … with 319,785 more rows, and 13 more variables: mental_health <dbl>,\n#   diff_walking <chr>, sex <chr>, age_category <chr>, race <chr>,\n#   diabetic <chr>, physical_activity <chr>, gen_health <chr>,\n#   sleep_time <dbl>, asthma <chr>, kidney_disease <chr>, skin_cancer <chr>,\n#   random_biomarker <dbl>\n\n\n\n\n\ndf.join(df2.set_index(\"gen_health\"), on='gen_health', how=\"right\")\n\n       heart_disease    bmi  ... skin_cancer random_biomarker\n10               Yes  34.30  ...          No        -1.085631\n25                No  29.18  ...          No        -1.085631\n35               Yes  32.98  ...         Yes        -1.085631\n45               Yes  20.37  ...          No        -1.085631\n49                No  27.46  ...          No        -1.085631\n...              ...    ...  ...         ...              ...\n319751            No  34.96  ...          No        -0.578600\n319772            No  30.90  ...          No        -0.578600\n319783            No  33.28  ...          No        -0.578600\n319788            No  23.38  ...          No        -0.578600\n319789            No  22.22  ...          No        -0.578600\n\n[319795 rows x 19 columns]"
  },
  {
    "objectID": "chapters/dm_nlp.html",
    "href": "chapters/dm_nlp.html",
    "title": "8  Natural Language Processing",
    "section": "",
    "text": "RPython\n\n\n\n# library(dplyr)\nlibrary(readr)\n# library(tidyr)\nlibrary(tidytext)\n\nrotam <- read_lines(\"https://www.gutenberg.org/cache/epub/151/pg151.txt\")\n\n\n\n\nimport pandas as pd\nimport nltk\nfrom urllib import request\n\nurl_rotam = request.urlopen(\"https://www.gutenberg.org/cache/epub/151/pg151.txt\")\nrotam = url_rotam.read().decode(\"utf8\")"
  },
  {
    "objectID": "chapters/dm_nlp.html#unnest-tokens",
    "href": "chapters/dm_nlp.html#unnest-tokens",
    "title": "8  Natural Language Processing",
    "section": "8.1 Unnest tokens",
    "text": "8.1 Unnest tokens\n\nrotam_df <- tibble::tibble(rotam) |> \n    mutate(flag_start = cumsum(str_detect(rotam, \"START\")),\n           flag_end = cumsum(str_detect(rotam, \"End\")),\n           chapter = cumsum(str_detect(rotam, \"PART THE\"))) |> \n    filter(flag_start == 1, flag_end == 0, chapter >= 1) |> \n    mutate(\n        stanza = cumsum(rotam == \"\" & (lead(rotam) != \"\" | !str_detect(rotam, \"PART THE\")))\n    ) |> \n    filter(!str_detect(rotam, \"PART THE\"), rotam != \"\") |> \n    select(-starts_with(\"flag\")) |> \n    mutate(verse = row_number(), rotam = str_squish(rotam) |> str_trim())\n\nrotam_words <- rotam_df |> unnest_tokens(word, rotam)\n\nrotam_words |> \n    anti_join(stop_words, by = \"word\") |> \n    count(word, chapter, sort = TRUE) |> \n    # left_join(rotam_words[c(\"chapter\", \"word\")], by = \"word\") |> \n    # distinct() |> \n    inner_join(get_sentiments(\"bing\"), by = \"word\") |> \n    mutate(n = ifelse(sentiment == \"negative\", -n, n)) |> \n    group_by(chapter) |> \n    slice_head(n = 10) |> \n    ungroup() |> \n    ggplot(aes(n, reorder(word, n), fill = sentiment)) +\n    geom_col() +\n    geom_text(aes(label = word), position = position_stack(vjust = 0.5), \n              color = \"grey\", size = 2) +\n    facet_grid(rows = vars(chapter), scales = \"free_y\") +\n    scale_fill_viridis_d() +\n    scale_x_continuous(breaks = seq(from = -10, to = 10, by = 1), \n                       labels = rlang::as_function(~ abs(.x))) +\n    theme_bw() +\n    labs(x = \"# of words\", y = NULL) +\n    theme(panel.grid.major.y = element_blank(),\n          axis.text.y = element_blank(), axis.ticks.y = element_blank())\n\n\nRPython\n\n\n\ntibble::tibble(rotam) |> \n    dplyr::filter() |> \n    unnest_tokens(word, rotam)\n\n\n\n\n# nltk.download(\"punkt\")\nrotam_words = nltk.word_tokenize(rotam)"
  },
  {
    "objectID": "chapters/dm_nlp.html#tf-idf",
    "href": "chapters/dm_nlp.html#tf-idf",
    "title": "8  Natural Language Processing",
    "section": "8.2 Tf-idf",
    "text": "8.2 Tf-idf"
  },
  {
    "objectID": "chapters/dm_nlp.html#document-term-matrix",
    "href": "chapters/dm_nlp.html#document-term-matrix",
    "title": "8  Natural Language Processing",
    "section": "8.3 Document-term matrix",
    "text": "8.3 Document-term matrix"
  },
  {
    "objectID": "chapters/p03_dv.html",
    "href": "chapters/p03_dv.html",
    "title": "Data Visualization",
    "section": "",
    "text": "The Seaborn “nextgen” API\n\n\n\nI decided to use the nextgen API of Seaborn because it has a lot of potential. Nonetheless, caution must be taken because it is in alpha release yet and everything can change. See here.\nIf you are comfortable with a ggplot2 syntax and grammar, then you can take a look at the plotnine Python package.\n\n\nThe data visualization part will cover the following topics:\n\nlook for trends in your data with scatter plots and line plots;\nvisualize distributions through bar plots, histograms, density plots, boxplots and violin plots;\naugment the information displayed in your plots with faceting;\ncustomize the theme of your plots."
  },
  {
    "objectID": "chapters/dv_trends.html",
    "href": "chapters/dv_trends.html",
    "title": "9  Trends",
    "section": "",
    "text": "We will keep using the heart disease dataset (see ?sec-import_local). However, we will consider a small subset of these data in order to lower computing resources."
  },
  {
    "objectID": "chapters/dv_trends.html#scatter-plots",
    "href": "chapters/dv_trends.html#scatter-plots",
    "title": "9  Trends",
    "section": "9.1 Scatter Plots",
    "text": "9.1 Scatter Plots\n\n9.1.1 A simple scatter plot\n\nRPython\n\n\n\nlibrary(ggplot2)\n\ndf |> \n    ggplot(aes(bmi, sleep_time)) +\n    geom_point()\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn.objects as so\n\nsns.set_theme()\n\n(\n    so.Plot(df, \"bmi\", \"sleep_time\")\n    .add(so.Scatter())\n    .show()\n)\n\n\n\n\n\n\n\n\n\n9.1.2 Color by variable\n\nRPython\n\n\n\ndf |> \n    ggplot(aes(bmi, sleep_time, color = heart_disease)) +\n    geom_point(alpha = 0.6) +\n    scale_color_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n(\n    so.Plot(df, \"bmi\", \"sleep_time\", color=\"heart_disease\")\n    .add(so.Scatter(alpha=0.6))\n    .scale(color=\"Set1\")\n    .show()\n)\n\n\n\n\nor also:\n\nsns.relplot(\n    x=\"bmi\", y=\"sleep_time\", hue=\"heart_disease\",\n    alpha=0.6,\n    palette=\"Set1\",\n    data=df\n);\nplt.show()"
  },
  {
    "objectID": "chapters/dv_trends.html#line-plots",
    "href": "chapters/dv_trends.html#line-plots",
    "title": "9  Trends",
    "section": "9.2 Line Plots",
    "text": "9.2 Line Plots\n\nRPython\n\n\n\ndf |> \n    ggplot(aes(bmi, sleep_time, color = heart_disease, linetype = smoking)) +\n    geom_line(alpha = 0.6) +\n    scale_color_viridis_d(option = \"plasma\")\n\n\n\n\n\n\n\nsns.lineplot(\n    x=\"bmi\", y=\"sleep_time\", hue=\"heart_disease\", style=\"smoking\",\n    alpha=0.6, \n    palette=\"plasma\",\n    data=df\n)"
  },
  {
    "objectID": "chapters/dv_trends.html#combine-different-layers",
    "href": "chapters/dv_trends.html#combine-different-layers",
    "title": "9  Trends",
    "section": "9.3 Combine different layers",
    "text": "9.3 Combine different layers\n\nRPython\n\n\n\ndf |> \n    ggplot(aes(bmi, sleep_time, color = heart_disease)) +\n    geom_line(aes(linetype = smoking), alpha = 0.6) +\n    geom_point(aes(shape = smoking), alpha = 0.6) +\n    scale_color_viridis_d(option = \"plasma\")\n\n\n\n\n\n\n\nsns.relplot(\n    x=\"bmi\", y=\"sleep_time\", hue=\"heart_disease\", style=\"smoking\",\n    kind=\"line\", markers=True, alpha=0.6, \n    palette=\"plasma\",\n    data=df\n);\nplt.show()"
  },
  {
    "objectID": "chapters/dv_distrib.html#bar-plots",
    "href": "chapters/dv_distrib.html#bar-plots",
    "title": "10  Distributions",
    "section": "10.1 Bar Plots",
    "text": "10.1 Bar Plots\n\n10.1.1 A simple bar plot\n\nRPython\n\n\n\ndf |> \n    ggplot(aes(gen_health)) +\n    geom_bar(color = \"red\")\n\n\n\n\n\n\n\nsns.countplot(x=\"gen_health\", color=\"grey\", edgecolor=\"red\", data=df)\n\n\n\n\n\n\n\n\n\n10.1.2 A stacked bar plot\n\nRPython\n\n\n\ndf |> \n    ggplot(aes(gen_health, fill = heart_disease)) +\n    geom_bar()\n\n\n\n\n\n\n\nsns.displot(x=\"gen_health\", hue=\"heart_disease\", multiple=\"stack\", aspect=7/5, data=df);\nplt.show()\n\n\n\n\n\n\n\n\n\n10.1.3 A dodged bar plot\n\nRPython\n\n\n\ndf |> \n    ggplot(aes(gen_health, fill = heart_disease)) +\n    geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\nsns.countplot(x=\"gen_health\", hue=\"heart_disease\", data=df)"
  },
  {
    "objectID": "chapters/dv_distrib.html#histograms",
    "href": "chapters/dv_distrib.html#histograms",
    "title": "10  Distributions",
    "section": "10.2 Histograms",
    "text": "10.2 Histograms\n\nRPython\n\n\n\ndf |> \n    ggplot(aes(bmi)) +\n    geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nsns.histplot(x=\"bmi\", data=df)"
  },
  {
    "objectID": "chapters/dv_distrib.html#density-plots",
    "href": "chapters/dv_distrib.html#density-plots",
    "title": "10  Distributions",
    "section": "10.3 Density Plots",
    "text": "10.3 Density Plots\n\nRPython\n\n\n\ndf |> \n    ggplot(aes(bmi)) +\n    geom_density()\n\n\n\n\n\n\n\nsns.kdeplot(x=\"bmi\", data=df)"
  },
  {
    "objectID": "chapters/dv_distrib.html#boxplots",
    "href": "chapters/dv_distrib.html#boxplots",
    "title": "10  Distributions",
    "section": "10.4 Boxplots",
    "text": "10.4 Boxplots\n\nRPython\n\n\n\ndf |> \n    ggplot(aes(smoking, bmi)) +\n    geom_boxplot()\n\n\n\n\n\n\n\nsns.boxplot(x=\"smoking\", y=\"bmi\", data=df)"
  },
  {
    "objectID": "chapters/dv_distrib.html#violin-plots",
    "href": "chapters/dv_distrib.html#violin-plots",
    "title": "10  Distributions",
    "section": "10.5 Violin Plots",
    "text": "10.5 Violin Plots\n\nRPython\n\n\n\ndf |> \n    ggplot(aes(smoking, bmi)) +\n    geom_violin()\n\n\n\n\n\n\n\nsns.violinplot(x=\"smoking\", y=\"bmi\", data=df)"
  },
  {
    "objectID": "chapters/p04_ml.html",
    "href": "chapters/p04_ml.html",
    "title": "Machine Learning",
    "section": "",
    "text": "RPython\n\n\n\nlibrary(tidymodels)\n\ndf <- readr::read_csv(\"heart_2020_cleaned.csv\", show_col_types = FALSE) |> \n    janitor::clean_names()\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom janitor import clean_names\n\ndf = pd.read_csv(\"heart_2020_cleaned.csv\").clean_names(case_type='snake')"
  },
  {
    "objectID": "chapters/ml_split.html",
    "href": "chapters/ml_split.html",
    "title": "11  Splitting Data",
    "section": "",
    "text": "RPython\n\n\n\nlibrary(tidymodels)\n\ndf <- readr::read_csv(\"data/heart_2020_cleaned.csv\", show_col_types = FALSE) |> \n    janitor::clean_names()\ndf <- df[1:1000, ]\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom janitor import clean_names\n\ndf = pd.read_csv(\"data/heart_2020_cleaned.csv\").clean_names(case_type='snake')\ndf = df.iloc[0:1000,]"
  },
  {
    "objectID": "chapters/ml_split.html#train-test-split",
    "href": "chapters/ml_split.html#train-test-split",
    "title": "11  Splitting Data",
    "section": "11.1 Train-test split",
    "text": "11.1 Train-test split\nTo prevent data leakage, one always have to train their model(s) (i.e. fit coefficients, find structure and association) on the training set and test it on the testing set (i.e. evaluate performance on never-seen data, generalization). Hence, the first step in the data modeling process is to split your data into two separate sets.\n\nRPython\n\n\n\nset.seed(42)\nsplits <- initial_split(df, prop = 0.8, strata = heart_disease)\ntrain_data <- training(splits)\ntest_data <- testing(splits)\n\n\n\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_data, test_data, train_target, test_target = train_test_split(\n    df.drop(columns=\"heart_disease\"), df[\"heart_disease\"],\n    train_size=0.8,\n    random_state=42\n)\n\n\n\n\n\n\n\n\n\n\nPseudo-random number generators\n\n\n\nProbability is concerned with studying uncertainty, randomness. Computers are not able to give you random answers, but they can simulate randomness with (pseudo) random number generators. In the following chapters, we will set the seeds of random number generators in order to reproduce the results (although some machine learning procedures involve randomness, you will be able to obtain the same results printed on this book).\nWe will use the same number (42, or The Answer to the Ultimate Question of Life, The Universe, and Everything) both in R and Python scripts, but remember that this choice does not lead to the same random process (even if the generation process is the same)."
  },
  {
    "objectID": "chapters/ml_split.html#cross-validation",
    "href": "chapters/ml_split.html#cross-validation",
    "title": "11  Splitting Data",
    "section": "11.2 Cross-validation",
    "text": "11.2 Cross-validation\n\n11.2.1 K-fold\n\nRPython\n\n\n\nset.seed(42)\ncv <- vfold_cv(train_data, v = 10)\n\n\n\n\nfrom sklearn.model_selection import KFold\n\ncv = KFold(n_splits=10, shuffle=True, random_state=42)\n\n\n\n\n\nStratified K-fold\n\nRPython\n\n\n\nset.seed(42)\ncv <- vfold_cv(train_data, v = 10, strata = heart_disease)\n\n\n\n\nfrom sklearn.model_selection import StratifiedKFold\n\ncv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n\n\n\n\n\n\nRepeated Stratified K-fold\n\nRPython\n\n\n\nset.seed(42)\ncv <- vfold_cv(train_data, v = 10, strata = heart_disease, repeats = 10)\n\n\n\n\nfrom sklearn.model_selection import RepeatedStratifiedKFold\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=42)\n\n\n\n\n\n\nGrouped K-fold\n\nRPython\n\n\n\ncv <- group_vfold_cv(train_data, group = , v = 10)\n\n\n\n\nfrom sklearn.model_selection import GroupKFold\n\ncv = GroupKFold(n_splits=10)\n# group is specified later\n\n\n\n\n\n\n\n11.2.2 Leave-one-out\n\nRPython\n\n\n\ncv <- loo_cv(train_data)\n\n\n\n\nfrom sklearn.model_selection import LeaveOneOut\n\ncv = LeaveOneOut()\n\n\n\n\n\n\n11.2.3 Monte Carlo\n\nRPython\n\n\n\nset.seed(42)\ncv <- mc_cv(train_data, prop = 3/4, times = 10)\n\n\n\n\nfrom sklearn.model_selection import ShuffleSplit\n\ncv = ShuffleSplit(n_splits=10, train_size=3/4, random_state=42)\n\n\n\n\n\nStratified Monte Carlo\n\nRPython\n\n\n\nset.seed(42)\ncv <- mc_cv(train_data, prop = 3/4, times = 10, strata = heart_disease)\n\n\n\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\ncv = StratifiedShuffleSplit(n_splits=10, train_size=3/4, random_state=42)\n\n\n\n\n\n\n\n11.2.4 Time Series split\n\nRPython\n\n\n\ncv <- rolling_origin(train_data, \n                     initial = (nrow(train_data) %% 11) + (nrow(train_data) %/% 11), \n                     assess = nrow(train_data) %/% 11, \n                     lag = 0)\n\n\n\n\nfrom sklearn.model_selection import TimeSeriesSplit\n\ncv = TimeSeriesSplit(n_splits=10, gap=0)"
  },
  {
    "objectID": "chapters/ml_prep.html",
    "href": "chapters/ml_prep.html",
    "title": "12  Preprocessing",
    "section": "",
    "text": "Pre-processing or feature engineering is a crucial step to take in every machine learning project. Some models require dummy variables instead of categorical ones, or normalized numerical variables, or also NA values to be imputed.\nIn tidymodels, every data pre-processing operation can be specified trough a step_ function of the recipes package. You start by defining a recipe object in which you specify the predictors and response variable as well as the data. Then, you add incrementally the step_ functions, which transform accordingly your data.\nIn Scikit-Learn, every pre-processor is a class object …"
  },
  {
    "objectID": "chapters/ml_prep.html#feature-engineering",
    "href": "chapters/ml_prep.html#feature-engineering",
    "title": "12  Preprocessing",
    "section": "12.1 Feature engineering",
    "text": "12.1 Feature engineering\n\nRPython\n\n\n\npreprocessor <- recipe(heart_disease ~ ., data = train_data) |> \n    step_normalize(all_numeric_predictors()) |> \n    step_dummy(all_nominal_predictors(), one_hot = FALSE)\n\n\npreprocessor |> \n    prep() |> \n    bake(new_data = NULL) |> \n    head()\n\n# A tibble: 6 × 38\n      bmi physical_health mental_health sleep_time heart_disease smoking_Yes\n    <dbl>           <dbl>         <dbl>      <dbl> <fct>               <dbl>\n1 -1.84           -0.0470         3.28     -1.46   No                      1\n2 -1.26           -0.424         -0.490    -0.0681 No                      0\n3 -0.275           2.09           3.28      0.626  No                      1\n4 -0.648          -0.424         -0.490    -0.762  No                      0\n5  0.0846          0.330         -0.490     3.40   Yes                     1\n6 -1.05            1.46          -0.490    -2.15   No                      0\n# … with 32 more variables: alcohol_drinking_Yes <dbl>, stroke_Yes <dbl>,\n#   diff_walking_Yes <dbl>, sex_Male <dbl>, age_category_X25.29 <dbl>,\n#   age_category_X30.34 <dbl>, age_category_X35.39 <dbl>,\n#   age_category_X40.44 <dbl>, age_category_X45.49 <dbl>,\n#   age_category_X50.54 <dbl>, age_category_X55.59 <dbl>,\n#   age_category_X60.64 <dbl>, age_category_X65.69 <dbl>,\n#   age_category_X70.74 <dbl>, age_category_X75.79 <dbl>, …\n\n\n\n\n\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import make_column_selector, ColumnTransformer\n\nselect_cat_cols = make_column_selector(dtype_include=object)\nselect_num_cols = make_column_selector(dtype_exclude=object)\n\npreprocessor = ColumnTransformer([\n    ('cat', OneHotEncoder(drop=\"first\", sparse=False), select_cat_cols(train_data)),\n    ('num', StandardScaler(), select_num_cols(train_data))\n])\n\n\npd.DataFrame(\n    preprocessor.fit_transform(train_data),\n    columns=preprocessor.get_feature_names_out(preprocessor.feature_names_in_)\n).head()\n\n   cat__smoking_Yes  ...  num__sleep_time\n0               1.0  ...        -0.065988\n1               1.0  ...        -0.763366\n2               0.0  ...         0.631390\n3               0.0  ...         0.631390\n4               1.0  ...        -0.065988\n\n[5 rows x 37 columns]"
  },
  {
    "objectID": "chapters/ml_prep.html#preprocessors",
    "href": "chapters/ml_prep.html#preprocessors",
    "title": "12  Preprocessing",
    "section": "12.2 Preprocessors",
    "text": "12.2 Preprocessors\nYou can find all the available recipes here. For Scikit-Learn, you can see the official API pages.\n\n12.2.1 Normalization\n\nz-score\n\nRPython\n\n\n\nstep_normalize()\nstep_center()\nstep_scale()\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nStandardScaler()\nStandardScaler(with_std=False)\nStandardScaler(with_mean=False)\n\n\n\n\n\n\nmin-max\n\nRPython\n\n\n\nstep_range()\nstep_range(min = -1, max = 1)\n\n\n\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nMinMaxScalerScaler()\nMinMaxScalerScaler(feature_range=(-1, 1))\n\n\n\n\n\n\nGaussian-like\n\nRPython\n\n\n\nstep_YeoJohnson()\nstep_BoxCox()\n\n\n\n\nfrom sklearn.preprocessing import PowerTransformer\n\nPowerTransformer(method=\"yeo-johnson\")\nPowerTransformer(method=\"box-cox\")\n\n\n\n\n\n\n\n12.2.2 Binning\n\nRPython\n\n\n\nstep_discretize(num_breaks = 4)\n\n\n\n\nfrom sklearn.preprocessing import KBinsDiscretizer\n\nKBinsDiscretizer(n_bins=5, encode=\"ordinal\")\n\n\n\n\n\n\n12.2.3 Dummy variables\n\nRPython\n\n\n\nstep_dummy(one_hot = TRUE)\nstep_dummy()\n\n\n\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nOneHotEncoder()\nOneHotEncoder(drop=\"first\")\n\n\n\n\n\n\n12.2.4 Imputation\n\nRPython\n\n\n\nstep_impute_mean()\nstep_impute_median()\nstep_impute_mode()\n\nstep_impute_knn(neighbors = 5)\n\n\n\n\nfrom sklearn.impute import SimpleImputer, KNNImputer\n\nSimpleImputer(strategy=\"mean\")\nSimpleImputer(strategy=\"median\")\nSimpleImputer(strategy=\"most_frequent\")\n\nKNNImputer(n_neighbors=5)\n\n\n\n\n\n\n12.2.5 Augmentation\n\nRPython\n\n\n\nstep_poly(degree=2)\nstep_bs(deg_free = 8, degree = 3)\n\n\n\n\nfrom sklearn.preprocessing import PolynomialFeatures, SplineTransformer\n\nPolynomialFeatures(degree=2)\nSplineTransformer(n_knots=5, degree=3, knots=\"quantile\")\n\n\n\n\n\n\n12.2.6 Apply functions\n\nRPython\n\n\n\nstep_log()\nstep_log(base = 10)\n\nstep_hyperbolic(func = \"sin\")\nstep_hyperbolic(func = \"cos\")\nstep_hyperbolic(func = \"tan\")\n\nstep_sqrt()\n\n\n\n\nimport numpy as np\nfrom sklearn.preprocessing import FunctionTransformer\n\nFunctionTransformer(func=np.log)\nFunctionTransformer(func=np.log10)\n\nFunctionTransformer(func=np.sin)\nFunctionTransformer(func=np.cos)\nFunctionTransformer(func=np.tan)\n\nFunctionTransformer(func=np.sqrt)\n\n\n\n\n\n\n12.2.7 Zero-variance\n\nRPython\n\n\n\nstep_zv()\n\n\n\n\nfrom sklearn.feature_selection import VarianceThreshold\n\nVarianceThreshold(threshold=0)"
  },
  {
    "objectID": "chapters/ml_fit.html#fitting-a-model",
    "href": "chapters/ml_fit.html#fitting-a-model",
    "title": "13  Fitting a Model",
    "section": "13.1 Fitting a model",
    "text": "13.1 Fitting a model"
  },
  {
    "objectID": "chapters/ml_fit.html#type-of-supervised-models",
    "href": "chapters/ml_fit.html#type-of-supervised-models",
    "title": "13  Fitting a Model",
    "section": "13.2 Type of (supervised) models",
    "text": "13.2 Type of (supervised) models\n\n13.2.1 Baseline\n\nRPython\n\n\n\nnull_model() |> set_mode(\"classification\")\nnull_model() |> set_mode(\"regression\")\n\n\n\n\nfrom sklearn.dummy import DummyClassifier, DummyRegressor\n\nDummyClassifier(strategy=\"most_common\")\nDummyRegressor(strategy=\"mean\")\n\n\n\n\n\n\n13.2.2 Linear Regression\n\nRPython\n\n\n\nRegression\n\nlinear_reg()\n\n\n\nClassification\n\nlogistic_reg()\n\n\n\n\n\nRegression\n\nfrom sklearn.linear_model import LinearRegression\n\nLinearRegression()\n\n\n\nClassification\n\nfrom sklearn.linear_model import LogisticRegression\n\nLogisticRegression(penalty=\"none\")\n\n\n\n\n\n\n\n13.2.3 Regularized Regression\n\nRPython\n\n\n\nRegression\n\nlinear_reg(penalty = 0.1, mixture = 0) |> set_mode(\"regression\")\nlinear_reg(penalty = 0.1, mixture = 1) |> set_mode(\"regression\")\nlinear_reg(penalty = 0.1, mixture = 0.5) |> set_mode(\"regression\")\n\n\n\nClassification\n\nlinear_reg(penalty = 0.1, mixture = 0) |> set_mode(\"classification\")\nlinear_reg(penalty = 0.1, mixture = 1) |> set_mode(\"classification\")\nlinear_reg(penalty = 0.1, mixture = 0.5) |> set_mode(\"classification\")\n\n\n\n\nParameters alpha (regularization strength) and C are linked by the following relationship: \\(\\alpha = \\frac{1}{2C}\\).\n\nRegression\n\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet\n\nRidge(alpha=0.1) # ElasticNet(alpha=0.1, l1_ratio=0)\nLasso(alpha=0.1) # ElasticNet(alpha=0.1, l1_ratio=1)\nElasticNet(alpha=0.1, l1_ratio=0.5)\n\n\n\nClassification\n\nfrom sklearn.linear_model import RidgeClassifier, LogisticRegression\n\nRidgeClassifier(alpha=0.1) # LogisticRegression(penalty=\"l2\", C=5)\nLogisticRegression(penalty=\"l1\", C=5)\nLogisticRegression(penalty=\"elasticnet\", C=5, l1_ratio=0.5)\n\n\n\n\n\n\n\n13.2.4 K-Nearest Neighbors\n\nRPython\n\n\n\nnearest_neighbor(neighbors = 5, dist_power = 2) |> set_mode(\"regression\")\nnearest_neighbor(neighbors = 5, dist_power = 2) |> set_mode(\"classification\")\n\n\n\n\nfrom sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n\nKNeighborsRegressor(n_neighbors=5, p=2)\nKNeighborsClassifier(n_neighbors=5, p=2)\n\n\n\n\n\n\n13.2.5 Support Vector Machine\n\nRPython\n\n\n\nRegression\n\nsvm_linear(cost = 5, margin = 0.01) |> set_mode(\"regression\")\nsvm_poly(cost = 5, degree = 3, margin = 0.01) |> set_mode(\"regression\")\nsvm_rbf(cost = 5, rbf_sigma = 20, margin = 0.01) |> set_mode(\"regression\")\n\n\n\nClassification\n\nsvm_linear(cost = 5) |> set_mode(\"classification\")\nsvm_poly(cost = 5, degree = 3) |> set_mode(\"classification\")\nsvm_rbf(cost = 5, rbf_sigma = 20) |> set_mode(\"classification\")\n\n\n\n\n\nRegression\n\nfrom sklearn.svm import LinearSVR, SVR\n\nLinearSVR(C=5, epsilon=0.01)\nSVR(kernel=\"poly\", C=5, degree=3, epsilon=0.01)\nSVR(kernel=\"rbf\", C=5, gamma=0.2, epsilon=0.01)\n\n\n\nClassification\n\nfrom sklearn.svm import LinearSVC, SVC\n\nLinearSVC(C=5)\nSVC(kernel=\"poly\", C=5, degree=3)\nSVC(kernel=\"rbf\", C=5, gamma=0.2)\n\n\n\n\n\n\n\n13.2.6 Classification and Regression Trees\n\nRPython\n\n\n\ndecision_tree() |> set_mode(\"regression\")\ndecision_tree() |> set_mode(\"classification\")\n\n\n\n\nfrom sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n\nDecisionTreeRegressor()\nDecisionTreeClassifier()\n\n\n\n\n\n\n13.2.7 Random Forest\n\nRPython\n\n\n\nrand_forest() |> set_mode(\"regression\")\nrand_forest() |> set_mode(\"classification\")\n\n\n\n\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n\nRandomForestRegressor()\nRandomForestClassifier()\n\n\n\n\n\n\n13.2.8 Bagging Trees\n\nRPython\n\n\n\nbag_tree() |> set_mode(\"regression\")\nbag_tree() |> set_mode(\"classification\")\n\n\n\n\nfrom sklearn.ensemble import BaggingRegressor, BaggingClassifier\nfrom sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n\nBaggingRegressor(base_estimator=DecisionTreeRegressor())\nBaggingClassifier(base_estimator=DecisionTreeClassifier())\n\n\n\n\n\n\n13.2.9 Gradient Boosting Machine\n\nRPython\n\n\n\nboost_tree() |> set_mode(\"regression\")\nboost_tree() |> set_mode(\"classification\")\n\n\n\n\nfrom sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n\nGradientBoostingRegressor()\nGradientBoostingClassifier()"
  }
]